{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision trees</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Goals\n",
    "\n",
    "* Learn how to create a regression tree model using scikit-learn\n",
    "* Show that a lot of data science is not always modeling but also looking at the data\n",
    "* Learn how to tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> What are regression trees?</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree used for regression. In this case of this notebook, we will use a regression tree to predict home prices. \n",
    "\n",
    "![image](images/regressionTree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Import Libraries</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this paritcular notebook you need to install folium if you want to run all the cells. \n",
    "\n",
    "Option 1:\n",
    "`pip install folium`\n",
    "\n",
    "Option 2 (Anaconda):\n",
    "`conda install -c conda-forge folium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "Kaggle hosts a dataset which contains house sales prices for King County, which includes Seattle.\n",
    "\n",
    "You can download the dataset from [Kaggle](https://www.kaggle.com/harlfoxem/housesalesprediction) or feel free to download it from my [GitHub](https://raw.githubusercontent.com/mGalarnyk/Tutorial_Data/master/King_County/kingCountyHouseData.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/mGalarnyk/Tutorial_Data/master/King_County/kingCountyHouseData.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>  Remove Missing or Impute Values </h2>\n",
    "If you want to build models with your data, null values are (almost) never allowed. It is important to always see how many samples have missing values and for which columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the shape of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a missing value in the Length column which is a feature\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exploratory data analysis</h2>\n",
    "\n",
    "When we looked for missing values, that was part of exploratory data analysis. Some other things you need to look out for are duplicated rows, outliers, and lack of documentation. Additionally, sometimes datasets have inaccuracies and you may need to consult subject matter experts to get their opinion on some oddities in the data. The reason why we do this before machine learning is that a common critical mistake in machine learning is simply to assume your data is good to work with and doesn't have any surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous_columns = ['price',\n",
    "                      'bedrooms',\n",
    "                      'bathrooms',\n",
    "                      'sqft_living',\n",
    "                      'sqft_lot',\n",
    "                      'floors',\n",
    "                      'waterfront',\n",
    "                      'view',\n",
    "                      'condition',\n",
    "                      'grade',\n",
    "                      'sqft_above',\n",
    "                      'sqft_basement',\n",
    "                      'yr_built',\n",
    "                      'yr_renovated',\n",
    "                      'zipcode',\n",
    "                      'lat',\n",
    "                      'long',\n",
    "                      'sqft_living15',\n",
    "                      'sqft_lot15']\n",
    "df.loc[:,continous_columns].hist(bins=25,figsize=(16,16),xlabelsize='10',ylabelsize='10',xrot=-15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at bedrooms, floors, bathrooms, and other variables vs price, I prefer boxplots because we have numerical data that is mostly not continuous. If you are curious what a boxplot is, I have an article on it [here](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51).\n",
    "\n",
    "From the charts below, it can be seen that there are some outliers like 33 bedrooms for a house and a price around 7000000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 3,\n",
    "                         ncols = 1,\n",
    "                         dpi=1000)\n",
    "\n",
    "sns.boxplot(x=df['bedrooms'],y=df['price'], ax=axes[0], showfliers = False)\n",
    "#axes[0].set_ylabel('')\n",
    "sns.boxplot(x=df['floors'],y=df['price'], ax=axes[1], showfliers = False)\n",
    "#axes[1].set_ylabel('')\n",
    "sns.boxplot(x=df['bathrooms'],y=df['price'], ax=axes[2], showfliers = False)\n",
    "axes[2].tick_params(axis = 'x', rotation = 90)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we have latitude and longtitude information for the houses. By using lat and long columns, I created the map below using the folium library which is a wrapper of a javascript library called [leaflet](https://leafletjs.com/reference-1.6.0.html#circlemarker-option). Notice that I didnt have equal sized bins because splitting the data into equal sized bins when the greatest house is 7,700,000 million and the least is 75,000 into even 6 bins means that each bin would cover more than a million. In the map, I didnt add a legend though I probably should ([link](https://stackoverflow.com/questions/37466683/create-a-legend-on-a-folium-map) to learn how to do it if curious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sidenote, look at the min and max price of home as well as most amount of bedrooms\n",
    "df['price'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for price\n",
    "(n, bins, patches) = plt.hist(df['price'].values,\n",
    "                              bins=6,\n",
    "                              edgecolor='black',\n",
    "                              linewidth=.9)\n",
    "plt.tick_params(axis = 'x', rotation = 90, labelsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The edges of the bins.\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for bedrooms\n",
    "(n, bins, patches) = plt.hist(df['bedrooms'].values,\n",
    "                              bins=6,\n",
    "                              edgecolor='black',\n",
    "                              linewidth=.9)\n",
    "plt.tick_params(axis = 'x', rotation = 90, labelsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The edges of the bins. Having bins this size is ridiculous. Should I have 33 bins?\n",
    "bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bin the histogram into quartiles so we can have some more balanced bins and reasonable colors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = df['price'].quantile([0,0.01, 0.25, 0.5, 0.75, 0.99, 1])\n",
    "df['price_bin'] = pd.cut(df['price'], bins = quantiles.values)\n",
    "\n",
    "# Removing left most house (cheapest)\n",
    "df = df.loc[~df['price_bin'].isna(), :]\n",
    "df['price_left'] = df['price_bin'].apply(lambda x: x.left)\n",
    "\n",
    "# Making color based on quantiles rather than equal size bins. \n",
    "hex_dict = {}\n",
    "for index,left in enumerate(df['price_left'].value_counts().sort_index().index.values):\n",
    "    hex_dict[left] = sns.color_palette(\"RdBu\", 6).as_hex()[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting the colors as hex because folium doesnt take tuples as inputs (for unknown reason)\n",
    "hex_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# You need to have installed folium to make this work\n",
    "# Creating Map\n",
    "startingmap = folium.Map(location=[47.5112, -122.257], control_scale=True, zoom_start=9.4)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    \n",
    "    price = int(row['price'])\n",
    "    bedrooms = row['bedrooms']\n",
    "    floors = row['floors']\n",
    "    bathrooms = row['bathrooms']\n",
    "    living = row['sqft_living']\n",
    "    waterfront = row['waterfront']\n",
    "    \n",
    "    popupinformation = ('Price: ' + \"{:,}\".format(price) + '<br>'\n",
    "                        'Bedrooms: ' + str(bedrooms) + '<br>'\n",
    "                        'Floors: ' + str(floors) + '<br>'\n",
    "                        'Bathrooms: ' + str(bathrooms) + '<br>'\n",
    "                        'Sqft_Living: ' + str(living) + '<br>'\n",
    "                        'Waterfront: ' + str(waterfront) + '<br>'\n",
    "                       )\n",
    "    \n",
    "    folium.CircleMarker([row['lat'], row['long']],\n",
    "                        color = hex_dict[row['price_left']],\n",
    "                        weight = .5,\n",
    "                        fill = True,\n",
    "                        fillColor = hex_dict[row['price_left']],\n",
    "                        popup = popupinformation,\n",
    "                        opacity = .3,\n",
    "                        fillOpacity = .3).add_to(startingmap)\n",
    "    \n",
    "startingmap.save('seattleMap.html')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clearly a relationship between location and price in this dataset but can a model that we build capture that. If we really want to make a good prediction, we could include additional information like schools in the area (like zillow) among many other things (distance to companies, more information on the homes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Arrange Data into Features Matrix and Target Vector </h2>\n",
    "Target is price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picked some features for now\n",
    "# See what happens if you input more or less features\n",
    "feature_names = ['bedrooms','bathrooms','sqft_living','sqft_lot','floors']\n",
    "\n",
    "X = df.loc[:, feature_names].values\n",
    "\n",
    "y = df.loc[:, 'price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train Test Split </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Decision tree regressor</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an instance of the Model.\n",
    "reg = DecisionTreeRegressor()\n",
    "\n",
    "# Training the model on the data, storing the information learned from the data\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "### Measure Model Performance\n",
    "score = reg.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.get_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Finding the Optimal max_depth</h2>\n",
    "Finding the optimal value for max_depth is one way to tune your model. The code below outputs the R^2 for regression trees with different values for max_depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to try for n_estimators:\n",
    "max_depth_range = list(range(1,80))\n",
    "\n",
    "# List to store the R2 for each value of max_depth\n",
    "score_list = []\n",
    "\n",
    "for depth in max_depth_range:\n",
    "    reg = DecisionTreeRegressor(max_depth = depth)\n",
    "    reg.fit(X_train, y_train)\n",
    "    score = reg.score(X_test, y_test)\n",
    "    score_list.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph below shows that the best R^2 for the model is when the hyperparameter max_depth is prepuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (10,7));\n",
    "ax.plot(max_depth_range,\n",
    "        score_list,\n",
    "        lw=2,\n",
    "        color='k')\n",
    "ax.set_xlim([1, max(max_depth_range)])\n",
    "ax.grid(True,\n",
    "        axis = 'both',\n",
    "        zorder = 0,\n",
    "        linestyle = ':',\n",
    "        color = 'k')\n",
    "ax.tick_params(labelsize = 18)\n",
    "ax.set_xlabel('max_depth', fontsize = 24)\n",
    "ax.set_ylabel('R^2', fontsize = 24)\n",
    "fig.tight_layout()\n",
    "#fig.savefig('images/max_depth_vs_R2.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>How do you create a visualization based on a decision tree? </h3>\n",
    "You can visualize decision trees using matplotlib, graphviz, or an online converter. You can read how to do it <a href=\"https://towardsdatascience.com/visualizing-decision-trees-with-python-scikit-learn-graphviz-matplotlib-1c50b4aa68dc\">here</a> (or use the notebook VisualizeDecisionTrees.ipynb to learn how to do it using matplotlib) In this example notice that petal width is the topmost split. It also happens to be the \"most important\" feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get depth\n",
    "reg.get_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please dont run this code\n",
    "# Depth of 39 is not probably not going to work the way you want it too. \n",
    "\"\"\"\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)\n",
    "tree.plot_tree(reg,\n",
    "               feature_names = feature_names,\n",
    "               filled = True);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>How can we make a better model</h3>\n",
    "A lot of machine learning is about data. It could be as simple as adding in more features. You could also use APIs to get more data (like distance to water, distance to stores, schools, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Which max_depth was best and why was the test R^2 so low</h3>\n",
    "The model overfit on the training set when it was allowed to grow without prepruning. Here is a graph to show the relationship between the train R^2 and the test R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "max_depth_list = []\n",
    "actual_depth_list = []\n",
    "train_score_list = []\n",
    "test_score_list = []\n",
    "\n",
    "for max_depth in range(1,80):\n",
    "    reg = DecisionTreeRegressor(max_depth = max_depth)\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    model_list.append(reg)\n",
    "    max_depth_list.append(max_depth)\n",
    "    actual_depth_list.append(reg.get_depth())\n",
    "    train_score_list.append(reg.score(X_train, y_train))\n",
    "    test_score_list.append(reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'model_list': model_list,\n",
    "             'max_depth': max_depth_list,\n",
    "             'actual_depth': actual_depth_list,\n",
    "             'train_score': train_score_list,\n",
    "             'test_score': test_score_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the best test score\n",
    "test_max = temp_df.loc[:, 'test_score'].max()\n",
    "temp_df.loc[temp_df.loc[:,'test_score'] == test_max, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the tree\n",
    "# kinda hard to read!\n",
    "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)\n",
    "tree.plot_tree(temp_df.loc[5, 'model_list'],\n",
    "               feature_names = feature_names,\n",
    "               filled = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 1)\n",
    "\n",
    "axes.plot(temp_df['max_depth'].values,\n",
    "          temp_df['test_score'].values,\n",
    "          label = 'Test Score')\n",
    "axes.plot(temp_df['max_depth'].values,\n",
    "          temp_df['train_score'].values,\n",
    "          label = 'Train Score',\n",
    "          color = 'r')\n",
    "axes.set_xlabel('max_depth', fontsize = 13)\n",
    "axes.set_ylabel('R^2', fontsize = 13)\n",
    "axes.set_title('Test vs Train R^2')\n",
    "axes.grid();\n",
    "axes.legend(loc = 'center right',\n",
    "            fontsize = 10)\n",
    "axes.set_title('Test vs Train R^2', fontsize = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dual axis graph below (no idea why I made it, but kinda like to show it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 1)\n",
    "\n",
    "axes.plot(temp_df['max_depth'].values,\n",
    "          temp_df['test_score'].values )\n",
    "axes.set_xlabel('max_depth')\n",
    "\n",
    "axes2=axes.twinx()\n",
    "axes2.plot(temp_df['max_depth'].values,temp_df['train_score'].values, color = 'r')\n",
    "axes2.tick_params('y', colors='r');\n",
    "\n",
    "trainLimits = temp_df['train_score'].min(), temp_df['train_score'].max() \n",
    "testLimits = temp_df['test_score'].min(), temp_df['test_score'].max() \n",
    "\n",
    "\n",
    "axes.set_ylim(testLimits)\n",
    "axes.set_yticks(np.linspace(testLimits[0],testLimits[1], 14))\n",
    "\n",
    "\n",
    "axes.set_ylabel('Test R^2')\n",
    "axes.set_title('Test vs Train R^2')\n",
    "\n",
    "axes2.set_ylim(trainLimits)\n",
    "axes2.set_yticks(np.linspace(trainLimits[0],trainLimits[1], 14))\n",
    "axes2.set_ylabel('Train R^2', color='r'); \n",
    "\n",
    "axes.grid();"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
