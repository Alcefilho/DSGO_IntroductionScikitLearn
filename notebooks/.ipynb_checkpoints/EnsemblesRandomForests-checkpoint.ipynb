{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles and Random Forests\n",
    "This notebook covers what bagging and random forests are and how to apply them using scikit-learn\n",
    "\n",
    "*Adapted from Chapter 8 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)* among many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Ensembling\n",
    "\n",
    "**Ensemble learning (or \"ensembling\")** is the process of combining several predictive models in order to produce a combined model that is more accurate than any individual model.\n",
    "\n",
    "- **Regression:** Take the average of the predictions.\n",
    "- **Classification:** Take a vote and use the most common prediction.\n",
    "\n",
    "For ensembling to work well, the models must be:\n",
    "\n",
    "- **Accurate:** They outperform the null model.\n",
    "- **Independent:** Their predictions are generated using different processes.\n",
    "\n",
    "**The big idea:** If you have a collection of individually imperfect (and independent) models, the \"one-off\" mistakes made by each model are probably not going to be made by the rest of the models, and thus the mistakes will be discarded when you average the models.\n",
    "\n",
    "There are two basic **methods for ensembling:**\n",
    "\n",
    "- Manually ensembling your individual models.\n",
    "- Using a model that ensembles for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as matplotlib\n",
    "\n",
    "# Visualize decision tree using matplotlib wrapper\n",
    "from sklearn import tree\n",
    "\n",
    "# Model imports\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set a threshold for which features to include.\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Metrics imports\n",
    "from sklearn import metrics\n",
    "\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "A weakness of **decision trees** is that they don't tend to have the best predictive accuracy. This is partially because of **high variance**, meaning that different splits in the training data can lead to very different trees.\n",
    "\n",
    "**Bagging** is a general-purpose procedure for reducing the variance of a machine learning method but is particularly useful for decision trees. Bagging is short for **bootstrap aggregation**, meaning the aggregation of bootstrap samples.\n",
    "\n",
    "A **bootstrap sample** is a random sample with replacement. So, it has the same size as the original sample but might duplicate some of the original observations. A sample in the case of bagged trees is a row of data. \n",
    "\n",
    "![baggedTreesImages](images/baggedTrees.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]\n",
      "[ 6 12 13  9 10 12  6 16  1 17  2 13  8 14  7 19  6 19 12 11]\n"
     ]
    }
   ],
   "source": [
    "# Set a seed for reproducibility.\n",
    "np.random.seed(1)\n",
    "\n",
    "# Create an array of 1 through 20.\n",
    "nums = np.arange(1, 21)\n",
    "print(nums)\n",
    "\n",
    "# Sample that array 20 times with replacement.\n",
    "print(np.random.choice(a=nums, size=20, replace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does bagging work (for decision trees)?**\n",
    "\n",
    "1. Grow N trees using N bootstrap samples from the training data. To reiterate, we randomly select samples from the original dataset. We can pick the same sample more than once. \n",
    "2. Train each tree on its bootstrap sample and make predictions.\n",
    "3. Combine the predictions:\n",
    "    * Average the predictions for **regression trees**.\n",
    "    * Take a vote for **classification trees**.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* **Each bootstrap sample** should be the same size as the original training set. (It may contain repeated rows.)\n",
    "* **N** should be a large enough value that the error seems to have \"stabilized\".\n",
    "* The trees are **grown deep** so that they have low bias/high variance.\n",
    "\n",
    "Bagging increases predictive accuracy by **reducing the variance**, similar to how cross-validation reduces the variance associated with train/test split (for estimating out-of-sample error) by splitting many times an averaging the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "The data we will use is the Breast Cancer Wisconsin (Diagnostic) Data Set: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic) which I converted to a csv for convenience. The goal of this prediction is successfully classifying cancer as malignant (1) or benign (0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wisconsinBreastCancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>mean_symmetry</th>\n",
       "      <th>mean_fractal_dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean_compactness  mean_concavity  mean_concave_points  mean_symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean_fractal_dimension  ...  worst_texture  worst_perimeter  worst_area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst_smoothness  worst_compactness  worst_concavity  worst_concave_points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst_symmetry  worst_fractal_dimension  diagnosis  \n",
       "0          0.4601                  0.11890          1  \n",
       "1          0.2750                  0.08902          1  \n",
       "2          0.3613                  0.08758          1  \n",
       "3          0.6638                  0.17300          1  \n",
       "4          0.2364                  0.07678          1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange Data into Features Matrix and Target Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[:, 'diagnosis'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([13,  2, 12,  2,  6,  1,  3, 10, 11,  9,  6,  1,  0,  1]),\n",
       " array([ 9,  0,  0,  9,  3, 13,  4,  0,  0,  4,  1,  7,  3,  2]),\n",
       " array([ 4,  7,  2,  4,  8, 13,  0,  7,  9,  3, 12, 12,  4,  6]),\n",
       " array([ 1,  5,  6, 11,  2,  1, 12,  8,  3, 10,  5,  0, 11,  2]),\n",
       " array([10, 10,  6, 13,  2,  4, 11, 11, 13, 12,  4,  6, 13,  3])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a seed for reproducibility.\n",
    "np.random.seed(123)\n",
    "\n",
    "# Create ten bootstrap samples (which will be used to select rows from the DataFrame).\n",
    "samples = [np.random.choice(a=14, size=14, replace=True) for _ in range(1, 6)]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_radius</th>\n",
       "      <th>mean_texture</th>\n",
       "      <th>mean_perimeter</th>\n",
       "      <th>mean_area</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>mean_compactness</th>\n",
       "      <th>mean_concavity</th>\n",
       "      <th>mean_concave_points</th>\n",
       "      <th>mean_symmetry</th>\n",
       "      <th>mean_fractal_dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst_radius</th>\n",
       "      <th>worst_texture</th>\n",
       "      <th>worst_perimeter</th>\n",
       "      <th>worst_area</th>\n",
       "      <th>worst_smoothness</th>\n",
       "      <th>worst_compactness</th>\n",
       "      <th>worst_concavity</th>\n",
       "      <th>worst_concave_points</th>\n",
       "      <th>worst_symmetry</th>\n",
       "      <th>worst_fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>15.85</td>\n",
       "      <td>23.95</td>\n",
       "      <td>103.70</td>\n",
       "      <td>782.7</td>\n",
       "      <td>0.08401</td>\n",
       "      <td>0.10020</td>\n",
       "      <td>0.09938</td>\n",
       "      <td>0.05364</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.05338</td>\n",
       "      <td>...</td>\n",
       "      <td>16.84</td>\n",
       "      <td>27.66</td>\n",
       "      <td>112.00</td>\n",
       "      <td>876.5</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.1924</td>\n",
       "      <td>0.2322</td>\n",
       "      <td>0.11190</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.06287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>19.17</td>\n",
       "      <td>24.80</td>\n",
       "      <td>132.40</td>\n",
       "      <td>1123.0</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.24580</td>\n",
       "      <td>0.20650</td>\n",
       "      <td>0.11180</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>...</td>\n",
       "      <td>20.96</td>\n",
       "      <td>29.94</td>\n",
       "      <td>151.70</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.3903</td>\n",
       "      <td>0.3639</td>\n",
       "      <td>0.17670</td>\n",
       "      <td>0.3176</td>\n",
       "      <td>0.10230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>16.02</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.03299</td>\n",
       "      <td>0.03323</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>19.19</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.1181</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>0.1459</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>15.78</td>\n",
       "      <td>17.89</td>\n",
       "      <td>103.60</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.09710</td>\n",
       "      <td>0.12920</td>\n",
       "      <td>0.09954</td>\n",
       "      <td>0.06606</td>\n",
       "      <td>0.1842</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>...</td>\n",
       "      <td>20.42</td>\n",
       "      <td>27.28</td>\n",
       "      <td>136.50</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>0.1396</td>\n",
       "      <td>0.5609</td>\n",
       "      <td>0.3965</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.3792</td>\n",
       "      <td>0.10480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>15.09</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_radius  mean_texture  mean_perimeter  mean_area  mean_smoothness  \\\n",
       "13        15.85         23.95          103.70      782.7          0.08401   \n",
       "2         19.69         21.25          130.00     1203.0          0.10960   \n",
       "12        19.17         24.80          132.40     1123.0          0.09740   \n",
       "2         19.69         21.25          130.00     1203.0          0.10960   \n",
       "6         18.25         19.98          119.60     1040.0          0.09463   \n",
       "1         20.57         17.77          132.90     1326.0          0.08474   \n",
       "3         11.42         20.38           77.58      386.1          0.14250   \n",
       "10        16.02         23.24          102.70      797.8          0.08206   \n",
       "11        15.78         17.89          103.60      781.0          0.09710   \n",
       "9         12.46         24.04           83.97      475.9          0.11860   \n",
       "6         18.25         19.98          119.60     1040.0          0.09463   \n",
       "1         20.57         17.77          132.90     1326.0          0.08474   \n",
       "0         17.99         10.38          122.80     1001.0          0.11840   \n",
       "1         20.57         17.77          132.90     1326.0          0.08474   \n",
       "\n",
       "    mean_compactness  mean_concavity  mean_concave_points  mean_symmetry  \\\n",
       "13           0.10020         0.09938              0.05364         0.1847   \n",
       "2            0.15990         0.19740              0.12790         0.2069   \n",
       "12           0.24580         0.20650              0.11180         0.2397   \n",
       "2            0.15990         0.19740              0.12790         0.2069   \n",
       "6            0.10900         0.11270              0.07400         0.1794   \n",
       "1            0.07864         0.08690              0.07017         0.1812   \n",
       "3            0.28390         0.24140              0.10520         0.2597   \n",
       "10           0.06669         0.03299              0.03323         0.1528   \n",
       "11           0.12920         0.09954              0.06606         0.1842   \n",
       "9            0.23960         0.22730              0.08543         0.2030   \n",
       "6            0.10900         0.11270              0.07400         0.1794   \n",
       "1            0.07864         0.08690              0.07017         0.1812   \n",
       "0            0.27760         0.30010              0.14710         0.2419   \n",
       "1            0.07864         0.08690              0.07017         0.1812   \n",
       "\n",
       "    mean_fractal_dimension  ...  worst_radius  worst_texture  worst_perimeter  \\\n",
       "13                 0.05338  ...         16.84          27.66           112.00   \n",
       "2                  0.05999  ...         23.57          25.53           152.50   \n",
       "12                 0.07800  ...         20.96          29.94           151.70   \n",
       "2                  0.05999  ...         23.57          25.53           152.50   \n",
       "6                  0.05742  ...         22.88          27.66           153.20   \n",
       "1                  0.05667  ...         24.99          23.41           158.80   \n",
       "3                  0.09744  ...         14.91          26.50            98.87   \n",
       "10                 0.05697  ...         19.19          33.88           123.80   \n",
       "11                 0.06082  ...         20.42          27.28           136.50   \n",
       "9                  0.08243  ...         15.09          40.68            97.65   \n",
       "6                  0.05742  ...         22.88          27.66           153.20   \n",
       "1                  0.05667  ...         24.99          23.41           158.80   \n",
       "0                  0.07871  ...         25.38          17.33           184.60   \n",
       "1                  0.05667  ...         24.99          23.41           158.80   \n",
       "\n",
       "    worst_area  worst_smoothness  worst_compactness  worst_concavity  \\\n",
       "13       876.5            0.1131             0.1924           0.2322   \n",
       "2       1709.0            0.1444             0.4245           0.4504   \n",
       "12      1332.0            0.1037             0.3903           0.3639   \n",
       "2       1709.0            0.1444             0.4245           0.4504   \n",
       "6       1606.0            0.1442             0.2576           0.3784   \n",
       "1       1956.0            0.1238             0.1866           0.2416   \n",
       "3        567.7            0.2098             0.8663           0.6869   \n",
       "10      1150.0            0.1181             0.1551           0.1459   \n",
       "11      1299.0            0.1396             0.5609           0.3965   \n",
       "9        711.4            0.1853             1.0580           1.1050   \n",
       "6       1606.0            0.1442             0.2576           0.3784   \n",
       "1       1956.0            0.1238             0.1866           0.2416   \n",
       "0       2019.0            0.1622             0.6656           0.7119   \n",
       "1       1956.0            0.1238             0.1866           0.2416   \n",
       "\n",
       "    worst_concave_points  worst_symmetry  worst_fractal_dimension  \n",
       "13               0.11190          0.2809                  0.06287  \n",
       "2                0.24300          0.3613                  0.08758  \n",
       "12               0.17670          0.3176                  0.10230  \n",
       "2                0.24300          0.3613                  0.08758  \n",
       "6                0.19320          0.3063                  0.08368  \n",
       "1                0.18600          0.2750                  0.08902  \n",
       "3                0.25750          0.6638                  0.17300  \n",
       "10               0.09975          0.2948                  0.08452  \n",
       "11               0.18100          0.3792                  0.10480  \n",
       "9                0.22100          0.4366                  0.20750  \n",
       "6                0.19320          0.3063                  0.08368  \n",
       "1                0.18600          0.2750                  0.08902  \n",
       "0                0.26540          0.4601                  0.11890  \n",
       "1                0.18600          0.2750                  0.08902  \n",
       "\n",
       "[14 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the rows for a bootstrapped sample \n",
    "X.iloc[samples[0], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 0,\n",
    "                                                    test_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagged Decision Trees in `scikit-learn` (with N = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1:</b> Import the model you want to use\n",
    "\n",
    "In sklearn, all machine learning models are implemented as Python classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2:</b> Make an instance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruct BaggingClassifier to use DecisionTreeClassifier as the \"base estimator\"\n",
    "# We are making 100 trees, bootstrapping our sample, and more\n",
    "bagclf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                          n_estimators=100,\n",
    "                          bootstrap=True,\n",
    "                          oob_score=True,\n",
    "                          random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3:</b> Training the model on the data, storing the information learned from the data. Model is learning the relationship between features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        presort=False,\n",
       "                                                        random_state=None,\n",
       "                                                        splitter='best'),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=1.0, n_estimators=100, n_jobs=None,\n",
       "                  oob_score=True, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 4:</b> Predict the labels of new data\n",
    "\n",
    "Uses the information the model learned during the model training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class predictions (not predicted probabilities)\n",
    "predictions = bagclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate classification accuracy\n",
    "score = bagclf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare your testing accuracy to the null accuracy\n",
    "Null accuracy is usually considered the accuracy obtained by always predicting the most frequent class.\n",
    "\n",
    "When interpreting the predictive power of a model, it's best to compare it to a baseline using a dummy model, sometimes called a baseline model. A dummy model is simply using the mean, median, or most common value as the prediction. This forms a benchmark to compare your model against and becomes especially important in classification where your null accuracy might be 95 percent.\n",
    "\n",
    "For example, suppose your dataset is **imbalanced** -- it contains 99% one class and 1% the other class. Then, your baseline accuracy (always guessing the first class) would be 99%. So, if your model is less than 99% accurate, you know it is worse than the baseline. Imbalanced datasets generally must be trained differently (with less of a focus on accuracy) because of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    67\n",
       "1    47\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_test)[0].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5877192982456141"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "67 / (67 + 47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this particular model has an accuracy of roughly x%. By comparison, the null accuracy was 58.77% for the split when this notebook was run. The model provides some value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing your Estimators\n",
    "You can select and visualization individual trees from a Bagged Tree (and Random Forests which we will learn about). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1028862084, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=870353631, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=788373214, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1419052930, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=873768326, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1622145301, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=2008179789, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=643033620, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1357834371, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1921671245, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=500268179, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=628835647, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1265743393, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=879215883, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1984193730, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1652356046, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=863374141, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1383220131, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=768526018, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=102072586, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1893800352, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1277932681, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1561103888, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=881471911, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=482591249, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1399666761, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1075722754, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=994184099, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1415918289, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=784270766, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1487562216, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1304288223, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1648716862, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1805327300, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=417014199, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=517700925, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1916502553, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=2115444460, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=882236800, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=60954093, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1629931859, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=346648370, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=696528854, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1755331698, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1389214709, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=256357650, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=531773512, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=618118544, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=702413022, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=191793590, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1058860009, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=714136488, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=933213625, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=335751472, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=203476569, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=14142466, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=952559688, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=497062937, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=812638173, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1962553096, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=451133823, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=2052448587, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=311105654, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1365326582, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=727957292, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=921524776, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1671693343, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1378888442, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=704149911, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=639143903, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=51853344, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=837432803, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1004400064, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1204390294, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1533105325, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1926613956, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1321471093, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1569646158, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=962761126, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1312685901, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=410742774, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=815497145, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1928987648, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=703187234, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=952443413, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1289658853, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=2082943737, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=910715732, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=716720943, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1385094845, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1889679661, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=879746494, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1002726665, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1873925657, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1561089986, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1783549193, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1207758436, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=341620552, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=394238580, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1030237385, splitter='best')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagclf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# We have 100 estimators\n",
    "print(len(bagclf.estimators_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=1028862084, splitter='best')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagclf.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This may not the best way to view each estimator as it is small \n",
    "plt.figure(figsize=(10,2))\n",
    "for index in range(0, 5):\n",
    "\n",
    "    plt.subplot(1, 5, 1 + index )\n",
    "    tree.plot_tree(bagclf.estimators_[index],\n",
    "               feature_names = X_train.columns, \n",
    "               class_names=['benign', 'malignant'],\n",
    "               filled = True);\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-25-1f9c240cb7ec>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-1f9c240cb7ec>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Ignore this cell. It is how to generate multiple images and combine\n",
    "# Beyond the scope of this course\n",
    "# https://stackoverflow.com/questions/30227466/combine-several-images-horizontally-with-python\n",
    "\n",
    "\"\"\"\n",
    "def concat_images(imga, imgb):\n",
    "    \"\"\"\n",
    "    #Combines two color image ndarrays side-by-side.\n",
    "    \"\"\"\n",
    "    ha,wa = imga.shape[:2]\n",
    "    hb,wb = imgb.shape[:2]\n",
    "    max_height = np.max([ha, hb])\n",
    "    total_width = wa+wb\n",
    "    new_img = np.zeros(shape=(max_height, total_width, 3))\n",
    "    new_img[:ha,:wa]=imga\n",
    "    new_img[:hb,wa:wa+wb]=imgb\n",
    "    return new_img\n",
    "\n",
    "def concat_n_images(image_path_list):\n",
    "    \"\"\"\n",
    "    #Combines N color images from a list of image paths.\n",
    "    \"\"\"\n",
    "    output = None\n",
    "    for i, img_path in enumerate(image_path_list):\n",
    "        img = plt.imread(img_path)[:,:,:3]\n",
    "        if i==0:\n",
    "            output = img\n",
    "        else:\n",
    "            output = concat_images(output, img)\n",
    "    return output\n",
    "\n",
    "image_list = []\n",
    "for index in range(0, 5):\n",
    "    filename = \"images/estimator\" +str(index).zfill(3)\n",
    "    tree.export_graphviz(bagclf.estimators_[index],\n",
    "                         out_file=filename + \".dot\",\n",
    "                         feature_names = X_train.columns, \n",
    "                         class_names=['benign', 'malignant'],\n",
    "                         filled = True)\n",
    "    call(['dot', '-Tpng', filename + \".dot\", '-o', filename + \".png\", '-Gdpi=1000'])\n",
    "    image_list.append(filename + '.png')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinedImages = concat_n_images(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib.image.imsave('images/5estimators.png', combinedImages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating out-of-sample error\n",
    "\n",
    "For bagged models, out-of-sample error can be estimated without using **train/test split** or **cross-validation**!\n",
    "\n",
    "On average, each bagged tree uses about **two-thirds** of the observations. For each tree, the **remaining observations** are called \"out-of-bag\" observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first bootstrap sample\n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below utilizes python sets since a property of sets is that they cannot have multiple occurrences of the same element. You can read more about them here: https://towardsdatascience.com/python-sets-and-set-theory-2ace093d1607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the \"in-bag\" observations for each sample\n",
    "for sample in samples:\n",
    "    print(set(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the \"out-of-bag\" observations for each sample\n",
    "for sample in samples:\n",
    "    print(sorted(set(range(14)).difference(set(sample))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know/forgot what a set difference is, the set returned from the difference can be visualized as the red part of the Venn diagram below.\n",
    "\n",
    "![](images/setdifference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to calculate **\"out-of-bag error\":**\n",
    "\n",
    "1. For every observation in the training data, predict its response value using **only** the trees in which that observation was out-of-bag. Average those predictions (for regression) or take a vote (for classification).\n",
    "2. Compare all predictions to the actual response values in order to compute the out-of-bag error.\n",
    "\n",
    "When N is sufficiently large, the **out-of-bag error** is an accurate estimate of **out-of-sample error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagclf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Advantages of bagged trees</b>\n",
    "\n",
    "Typically better performance than decision trees\n",
    "\n",
    "Can be used for classification or regression\n",
    "\n",
    "Don't require feature scaling\n",
    "\n",
    "They allow you to estimate out-of-sample error without using train/test split or cross-validation.\n",
    "\n",
    "<b>Disadvantages of bagged trees</b>\n",
    "\n",
    "They are less interpretable than decision trees\n",
    "\n",
    "They are slower to train\n",
    "\n",
    "They are slower to predict (not much of a problem)\n",
    "\n",
    "Individual models can be correlated with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Random Forests offer a variation on bagged trees with potentially better performance. Bagging tends to reduce the variance and random forests try to further reduce the variance. Suppose there is **one very strong feature** in the data set. When using bagged trees, most of the trees could use that feature as the top split, resulting in an ensemble of similar trees that are **highly correlated**. Averaging highly correlated quantities does might not significantly reduce variance (which is the entire goal of bagging). By randomly leaving out candidate features from each split, **Random Forests \"decorrelates\" the trees**, such that the averaging process can reduce the variance of the resulting model.\n",
    "\n",
    "![baggedTreesImages](images/randomForest.png)\n",
    "\n",
    "**How do random forests work?**\n",
    "\n",
    "1. Grow N trees using N bootstrap samples from the training data. However, when building each tree, each time a split is considered, a **random sample of m features** is chosen as split candidates from the **full set of p features**. The split is only allowed to use **one of those m features**. A new random sample of features is chosen for **every single tree at every single split**.\n",
    "2. Train each tree on its bootstrap sample and make predictions.\n",
    "3. Combine the predictions:\n",
    "    * Average the predictions for **regression trees**.\n",
    "    * Take a vote for **classification trees**.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* **Each bootstrap sample** should be the same size as the original training set. (It may contain repeated rows.)\n",
    "* **N** should be a large enough value that the error seems to have \"stabilized\".\n",
    "* The trees are **grown deep** so that they have low bias/high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Advantages of random forests</b>\n",
    "\n",
    "Typically better performance than decision trees\n",
    "\n",
    "Can be used for classification or regression\n",
    "\n",
    "Don't require feature scaling\n",
    "\n",
    "They allow you to estimate out-of-sample error without using train/test split or cross-validation.\n",
    "\n",
    "<b>Disadvantages of random forests</b>\n",
    "\n",
    "They are less interpretable than decision trees\n",
    "\n",
    "They are slower to train\n",
    "\n",
    "They are slower to predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests in `scikit-learn` (with N = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1:</b> Import the model you want to use\n",
    "\n",
    "In sklearn, all machine learning models are implemented as Python classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2:</b> Make an instance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                             bootstrap=True,\n",
    "                             oob_score=True,\n",
    "                             random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3:</b> Training the model on the data, storing the information learned from the data. Model is learning the relationship between features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 4:</b> Predict the labels of new data\n",
    "\n",
    "Uses the information the model learned during the model training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class predictions (not predicted probabilities)\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate classification accuracy\n",
    "score = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the score here is similar to what we had before for bagged trees, keep in mind that this is a small dataset (relatively speaking) and we haven't hyperparameter tuned our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning n_estimators\n",
    "\n",
    "A tuning parameter is **n_estimators**, which represents the number of trees that should be grown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble.forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is always the same as the original\n",
      " |  input sample size but the samples are drawn with replacement if\n",
      " |  `bootstrap=True` (default).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |         The default value of ``n_estimators`` will change from 10 in\n",
      " |         version 0.20 to 100 in version 0.22.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"gini\")\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, (default=1e-7)\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole datset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool (default=False)\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization accuracy.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or     None, optional (default=None)\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_decision_function_ : array of shape = [n_samples, n_classes]\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  \n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
      " |  ...                              random_state=0)\n",
      " |  >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE\n",
      " |  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      " |              max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      " |              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      " |              min_samples_leaf=1, min_samples_split=2,\n",
      " |              min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      " |              oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      " |  >>> print(clf.feature_importances_)\n",
      " |  [0.14205973 0.76664038 0.0282433  0.06305659]\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      BaseForest\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest. The\n",
      " |      class probability of a single tree is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute `classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |      \n",
      " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[1] + list(range(10, 310, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to try for n_estimators:\n",
    "estimator_range = [1] + list(range(10, 310, 10))\n",
    "\n",
    "# List to store the average RMSE for each value of n_estimators:\n",
    "scores = []\n",
    "\n",
    "# Use five-fold cross-validation with each value of n_estimators (Warning: Slow!).\n",
    "for estimator in estimator_range:\n",
    "    clf = RandomForestClassifier(n_estimators=estimator, random_state=1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.plot(estimator_range, scores);\n",
    "\n",
    "plt.xlabel('n_estimators', fontsize =20);\n",
    "plt.ylabel('Accuracy', fontsize = 20);\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At some point there looks to be diminishing returns for accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Which model is best?** The best classifier for a particular task is task-dependent. In many business cases, interpretability is more important than accuracy. So, decision trees may be preferred. In other cases, accuracy on unseen data might be paramount, in which case random forests would likely be better (since they typically overfit less). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
