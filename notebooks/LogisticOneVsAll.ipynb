{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Versus All and One Versus One \n",
    "A lot of this notebook was based on [Machine Learning Mastery: OvR, OvO](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/). This was used for the Q/A session to answer a students question for Data Science Go Virtual October 25, 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import seaborn as sns\n",
    "import math\n",
    "from IPython.display import Video\n",
    "\n",
    "# Model imports\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "# Metrics imports\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many linear classification models (like logistic regression, support vector machines) are for binary classification only, and don't extend naturally to the multiclass case (with the exception of logistic regression) where multiclass means more than two classes. \n",
    "\n",
    "One approach for using binary classification algorithms for multi-classification problems is to split the multi-class classification dataset into multiple binary classification datasets and fit a binary classification model on each. Two different examples of this approach are the One-vs-Rest (OvR), also known as One-vs-All (OvA) and One-vs-One (OvO) strategies.\n",
    "\n",
    "What is covered in this notebook is the following: \n",
    "\n",
    "* The One-vs-Rest (One-vs-All) strategy splits a multi-class classification into one binary classification problem per class.\n",
    "* The One-vs-One strategy splits a multi-class classification into one binary classification problem per each pair of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Versus All (OvA)\n",
    "One Versus All, which is sometimes also called one versus rest (OvR) is a technique that allows us to extend any binary classifier to multi-class problems. We can train one classifier per class, where the particular class is treated as the positive class and the examples from all other classes are considered negative classes. If we were to classify a new, unlabeled data instance, we would use our n classifiers, where n is the number of class labels, and assign the class label the highest confidence to the particular instance we want to classify. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Versus All Theoretical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, given a multi-class classification problem with examples for each class ‘setosa,’ ‘versicolor,’ and ‘virginica‘. This could be divided into three binary classification datasets as follows:\n",
    "\n",
    "Binary Classification Problem 1: setosa vs [versicolor, virginica]\n",
    "\n",
    "Binary Classification Problem 2: versicolor vs [setosa, virginica]\n",
    "\n",
    "Binary Classification Problem 3: virginica vs [setosa, versicolor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach requires that each model predicts a class membership probability or a probability-like score. The argmax of these scores (class index with the largest score) is then used to predict a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is commonly used for algorithms that naturally predict numerical class membership probability or score, such as logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, the implementation of these algorithms in the scikit-learn library implements the OvR strategy by default when using these algorithms for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can demonstrate this with an example on a 3-class classification problem using the LogisticRegression algorithm. The strategy for handling multi-class classification can be set via the “multi_class” argument and can be set to “ovr” for the one-vs-rest strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete example of fitting a logistic regression model for multi-class classification using the built-in one-vs-rest strategy is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression for multi-class classification using built-in one-vs-rest\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
    "# define model\n",
    "model = LogisticRegression(multi_class='ovr')\n",
    "# fit model\n",
    "model.fit(X, y)\n",
    "# make predictions\n",
    "yhat = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn library also provides a separate [OneVsRestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) class that allows the one-vs-rest strategy to be used with any classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class can be used to use a binary classifier like Logistic Regression or another algorithm for multi-class classification, or even other classifiers that natively support multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very easy to use and requires that a classifier that is to be used for binary classification be provided to the OneVsRestClassifier as an argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very easy to use and requires that a classifier that is to be used for binary classification be provided to the OneVsRestClassifier as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression for multi-class classification using a one-vs-rest\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
    "# define model\n",
    "model = LogisticRegression()\n",
    "# define the ovr strategy\n",
    "ovr = OneVsRestClassifier(model)\n",
    "# fit model\n",
    "ovr.fit(X, y)\n",
    "# make predictions\n",
    "yhat = ovr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Versus All Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "col_names = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n",
    "                   'Proline']\n",
    "\n",
    "df = pd.read_csv(r'data/wine.data',\n",
    "                     header = None,\n",
    "                names = col_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class label</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class label  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0            1    14.23        1.71  2.43               15.6        127   \n",
       "1            1    13.20        1.78  2.14               11.2        100   \n",
       "2            1    13.16        2.36  2.67               18.6        101   \n",
       "3            1    14.37        1.95  2.50               16.8        113   \n",
       "4            1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Print out how many classes\n",
    "print('Class labels', np.unique(df['Class label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    71\n",
       "1    59\n",
       "3    48\n",
       "Name: Class label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classes aren't balanced.\n",
    "df['Class label'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange data into features matrix and target vector\n",
    "X = df.loc[:, df.columns[(df.columns != 'Class label')]]\n",
    "y = df.loc[:, 'Class label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In statistical surveys, \n",
    "# when subpopulations within an overall population vary,\n",
    "# it could be advantageous to sample each subpopulation (stratum) independently. Stratification is the process of dividing members of the population into homogeneous subgroups before sampling.\n",
    "#help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets \n",
    "# Providing the class label array y as an argument to stratify ensures both\n",
    "# the training set and test datasets have the same class proportions as the\n",
    "# original dataset\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   test_size=0.3, \n",
    "                                                   random_state=0, \n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 41, 2: 50, 3: 33}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(penalty='l1',\n",
    "                        C=1.0,\n",
    "                        solver='liblinear',\n",
    "                        multi_class='ovr')\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "print('Training accuracy:', log_reg.score(X_train, y_train))\n",
    "print('Test accuracy:', log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the training and test accuracies (both 100 percent) indicate that our model does a perfect job on both datasets. When you access the intercept terms via the `log_reg.intercept_` attribute, we see that the array returns three values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.26389183, -1.2159549 , -2.36959333])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we fit the Logistic Regression object on a multiclass dataset via the OvR approach, the first intercept belongs to the model that fits class 1 versus classes 2 and 3, the second value is the intercept of the model that fits class 2 versus classes 1 and 3, and the third value is the intercept of the model that fits class 3 versus  1 and 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.24566275,  0.18064686,  0.74592289, -1.16393436,  0.        ,\n",
       "         0.        ,  1.16077801,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.55718013,  2.50873345],\n",
       "       [-1.53705281, -0.38729034, -0.99526575,  0.36483503, -0.05932216,\n",
       "         0.        ,  0.66810689,  0.        ,  0.        , -1.93415219,\n",
       "         1.23404742,  0.        , -2.23179996],\n",
       "       [ 0.13525564,  0.16995133,  0.35782592,  0.        ,  0.        ,\n",
       "         0.        , -2.43087034,  0.        ,  0.        ,  1.56186001,\n",
       "        -0.8189804 , -0.49717816,  0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight array that is accessed by `log_reg.coef_` attribute that contains three rows of weight coefficients, one weight vector for each class. Each row consists of 13 weights, where each weight is multiplied by the respective feature in the 13-dimensional wine dataset to calculated the net input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "You choose the highest probability of all the class probabilties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first row of features\n",
    "X_test[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first label in the test set\n",
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.89443737, -0.38811788,  1.10073064, -0.81201711,  1.13201117,\n",
       "        1.09807851,  0.71204102,  0.18101342,  0.06628046,  0.51285923,\n",
       "        0.79629785,  0.44829502,  1.90593792])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.76557984e-01, 4.48247143e-04, 2.29937690e-02]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first class is the highest score so it will be the predict for this data\n",
    "log_reg.predict_proba(X_test[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method predict_proba in module sklearn.linear_model._logistic:\n",
      "\n",
      "predict_proba(X) method of sklearn.linear_model._logistic.LogisticRegression instance\n",
      "    Probability estimates.\n",
      "    \n",
      "    The returned estimates for all classes are ordered by the\n",
      "    label of classes.\n",
      "    \n",
      "    For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      "    the softmax function is used to find the predicted probability of\n",
      "    each class.\n",
      "    Else use a one-vs-rest approach, i.e calculate the probability\n",
      "    of each class assuming it to be positive using the logistic function.\n",
      "    and normalize these values across all the classes.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like of shape (n_samples, n_features)\n",
      "        Vector to be scored, where `n_samples` is the number of samples and\n",
      "        `n_features` is the number of features.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    T : array-like of shape (n_samples, n_classes)\n",
      "        Returns the probability of the sample for each class in the model,\n",
      "        where classes are ordered as they are in ``self.classes_``.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(log_reg.predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Versus One Theoretical Example (Not my Favorite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not my favorite method to teach for this particular course since we don't cover support vector machines which this is commonly used for, but I am including it for your information.\n",
    "\n",
    "Classically, this approach is suggested for support vector machines (SVM) and related kernel-based algorithms. This is believed because the performance of kernel methods does not scale in proportion to the size of the training dataset and using subsets of the training data may counter this effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-vs-One (OvO for short) is another heuristic method for using binary classification algorithms for multi-class classification.\n",
    "\n",
    "Like one-vs-rest, one-vs-one splits a multi-class classification dataset into binary classification problems. Unlike one-vs-rest that splits it into one binary dataset for each class, the one-vs-one approach splits the dataset into one dataset for each class versus every other class.\n",
    "\n",
    "For example, consider a multi-class classification problem with four classes: ‘red,’ ‘blue,’ and ‘green,’ ‘yellow.’ This could be divided into six binary classification datasets as follows:\n",
    "\n",
    "* Binary Classification Problem 1: red vs. blue\n",
    "\n",
    "* Binary Classification Problem 2: red vs. green\n",
    "\n",
    "* Binary Classification Problem 3: red vs. yellow\n",
    "\n",
    "* Binary Classification Problem 4: blue vs. green\n",
    "\n",
    "* Binary Classification Problem 5: blue vs. yellow\n",
    "\n",
    "* Binary Classification Problem 6: green vs. yellow\n",
    "\n",
    "This is significantly more datasets, and in turn, models than the one-vs-rest strategy described in the previous section.\n",
    "\n",
    "The formula for calculating the number of binary datasets, and in turn, models, is as follows:\n",
    "\n",
    "* (NumClasses * (NumClasses – 1)) / 2\n",
    "\n",
    "We can see that for four classes, this gives us the expected value of six binary classification problems:\n",
    "\n",
    "(* NumClasses * (NumClasses – 1)) / 2\n",
    "* (4 * (4 – 1)) / 2\n",
    "* (4 * 3) / 2\n",
    "* 12 / 2\n",
    "* 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each binary classification model may predict one class label and the model with the most predictions or votes is predicted by the one-vs-one strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"An alternative is to introduce K(K − 1)/2 binary discriminant functions, one for every possible pair of classes. This is known as a one-versus-one classifier. Each point is then classified according to a majority vote amongst the discriminant functions.\" — Page 183, [Pattern Recognition and Machine Learning](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=as_li_ss_tl?keywords=Pattern+Recognition+and+Machine+Learning&qid=1579729404&sr=8-1&linkCode=sl1&tag=inspiredalgor-20&linkId=35644770788d3f7d6402ac21dac68952&language=en_US), 2006.\n",
    "\n",
    "Similarly, if the binary classification models predict a numerical class membership, such as a probability, then the argmax of the sum of the scores (class with the largest sum score) is predicted as the class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector machine implementation in the scikit-learn is provided by the SVC class and supports the one-vs-one method for multi-class classification problems. This can be achieved by setting the “decision_function_shape” argument to ‘ovo‘."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below demonstrates SVM for multi-class classification using the one-vs-one method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM for multi-class classification using built-in one-vs-one\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
    "# define model\n",
    "model = SVC(decision_function_shape='ovo')\n",
    "# fit model\n",
    "model.fit(X, y)\n",
    "# make predictions\n",
    "yhat = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn library also provides a separate [OneVsOneClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html) class that allows the one-vs-one strategy to be used with any classifier.\n",
    "\n",
    "This class can be used with a binary classifier like SVM, Logistic Regression or Perceptron for multi-class classification, or even other classifiers that natively support multi-class classification.\n",
    "\n",
    "It is very easy to use and requires that a classifier that is to be used for binary classification be provided to the OneVsOneClassifier as an argument.\n",
    "\n",
    "The example below demonstrates how to use the OneVsOneClassifier class with an SVC class used as the binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM for multi-class classification using one-vs-one\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
    "# define model\n",
    "model = SVC()\n",
    "# define ovo strategy\n",
    "ovo = OneVsOneClassifier(model)\n",
    "# fit model\n",
    "ovo.fit(X, y)\n",
    "# make predictions\n",
    "yhat = ovo.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Machine Learning Mastery: OvR, OvO](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)\n",
    "\n",
    "[Scikit-Learn: One vs One](https://scikit-learn.org/stable/modules/multiclass.html#ovo-classification)\n",
    "\n",
    "[Coursera: One vs All (Andrew Ng)](https://www.coursera.org/lecture/machine-learning/multiclass-classification-one-vs-all-68Pol)\n",
    "\n",
    "If you can't view the coursera one, you can open it in incognito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
