{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook Goals\n",
    "\n",
    "* Learn how to create a logistic regression model using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> What are some advantages of logistic regression?</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you create a logistic regression model using Scikit-Learn? The first thing you need to know is that despite the name logistic regression containing the word regression, logistic regression is a model used for classification. Classification models can be used for tasks like classifying flower species or image recognition. All of this of course depends on the availability and quality of your data. Logistic Regression has some advantages including\n",
    "\n",
    "* Model training and predictions are relatively fast\n",
    "* No tuning is usually needed for logistic regression unless you want to regularize your model. \n",
    "* Finally, it can perform well with a small number of observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Import Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import Video\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "The Iris dataset is one of datasets scikit-learn comes with that do not require the downloading of any file from some external website. The code below loads the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/modifiedIris2Classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>  Remove Missing or Impute Values </h2>\n",
    "If you want to build models with your data, null values are (almost) never allowed. It is important to always see how many samples have missing values and for which columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the shape of the dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a missing value in the Length column which is a feature\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train Test Split </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[['petal length (cm)']], df['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Standardize the Data</h2>\n",
    "Logistic Regression is effected by scale so you need to scale the features in the data before using Logistic Regresison. You can transform the data onto unit scale (mean = 0 and variance = 1) for better performance. Scikit-Learn's `StandardScaler` helps standardize the dataset’s features. Note you fit on the training set and transform on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Logistic Regression</h2>\n",
    "\n",
    "<b>Step 1:</b> Import the model you want to use\n",
    "\n",
    "In sklearn, all machine learning models are implemented as Python classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was already imported earlier in the notebook so commenting out\n",
    "#from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2:</b> Make an instance of the Model\n",
    "\n",
    "This is a place where we can tune the hyperparameters of a model. Typically this is where you tune C which is related to regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3:</b> Training the model on the data, storing the information learned from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is learning the relationship between x (features sepal width, sepal height etc) and y (labels-which species of iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 4:</b> Predict the labels of new data (new flowers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression also allows you to see prediction probabilities as well as  a prediction. This is not like other algorithms like decision trees for classification which only give you a prediction not a probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One observation's petal length after standardization\n",
    "X_test[0].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('prediction', clf.predict(X_test[0].reshape(1,-1))[0])\n",
    "print('probability', clf.predict_proba(X_test[0].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is unclear, let's visualize how logistic regression makes predictions by looking at our test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df = pd.DataFrame()\n",
    "example_df.loc[:, 'petal length (cm)'] = X_test.reshape(-1)\n",
    "example_df.loc[:, 'target'] = y_test.values\n",
    "example_df['logistic_preds'] = pd.DataFrame(clf.predict_proba(X_test))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (10,7));\n",
    "\n",
    "\n",
    "virginicaFilter = example_df['target'] == 1\n",
    "versicolorFilter = example_df['target'] == 0\n",
    "\n",
    "ax.scatter(example_df.loc[virginicaFilter, 'petal length (cm)'].values,\n",
    "            example_df.loc[virginicaFilter, 'logistic_preds'].values,\n",
    "           color = 'g',\n",
    "           s = 60,\n",
    "           label = 'virginica')\n",
    "\n",
    "\n",
    "ax.scatter(example_df.loc[versicolorFilter, 'petal length (cm)'].values,\n",
    "            example_df.loc[versicolorFilter, 'logistic_preds'].values,\n",
    "           color = 'b',\n",
    "           s = 60,\n",
    "           label = 'versicolor')\n",
    "\n",
    "ax.axhline(y = .5, c = 'y')\n",
    "\n",
    "ax.axhspan(.5, 1, alpha=0.05, color='green')\n",
    "ax.axhspan(0, .4999, alpha=0.05, color='blue')\n",
    "ax.text(0.5, .6, 'Classified as viginica', fontsize = 16)\n",
    "ax.text(0.5, .4, 'Classified as versicolor', fontsize = 16)\n",
    "\n",
    "ax.set_ylim(0,1)\n",
    "ax.legend(loc = 'lower right', markerscale = 1.0, fontsize = 12)\n",
    "ax.tick_params(labelsize = 18)\n",
    "ax.set_xlabel('petal length (cm)', fontsize = 24)\n",
    "ax.set_ylabel('probability of virginica', fontsize = 24)\n",
    "ax.set_title('Logistic Regression Predictions', fontsize = 24)\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Measuring Model Performance</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are other ways of measuring model performance (precision, recall, F1 Score, ROC Curve, etc), let's keep this simple and use accuracy as our metric. \n",
    "To do this are going to see how the model performs on new data (test set)\n",
    "\n",
    "Accuracy is defined as:\n",
    "(fraction of correct predictions): correct predictions / total number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is one metric, but it doesn't say give much insight into what was wrong. Let's look at a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, clf.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True,\n",
    "            fmt=\".0f\",\n",
    "            linewidths=.5,\n",
    "            square = True,\n",
    "            cmap = 'Blues');\n",
    "plt.ylabel('Actual label', fontsize = 17);\n",
    "plt.xlabel('Predicted label', fontsize = 17);\n",
    "plt.title('Accuracy Score: {}'.format(score), size = 17);\n",
    "plt.tick_params(labelsize= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>What went wrong with the confusion matrix? It looks bad!</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, clf.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True,\n",
    "            fmt=\".0f\",\n",
    "            linewidths=.5,\n",
    "            square = True,\n",
    "            cmap = 'Blues');\n",
    "plt.ylabel('Actual label', fontsize = 17);\n",
    "plt.xlabel('Predicted label', fontsize = 17);\n",
    "plt.title('Accuracy Score: {}'.format(score), size = 17);\n",
    "plt.tick_params(labelsize= 15)\n",
    "\n",
    "# You can comment out the next 4 lines if you like\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the same information in a table in a clearer way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore this code\n",
    "\n",
    "modified_cm = []\n",
    "for index,value in enumerate(cm):\n",
    "    if index == 0:\n",
    "        modified_cm.append(['TN = ' + str(value[0]), 'FP = ' + str(value[1])])\n",
    "    if index == 1:\n",
    "        modified_cm.append(['FN = ' + str(value[0]), 'TP = ' + str(value[1])])   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=np.array(modified_cm),\n",
    "            fmt=\"\",\n",
    "            annot_kws={\"size\": 20},\n",
    "            linewidths=.5,\n",
    "            square = True,\n",
    "            cmap = 'Blues',\n",
    "            xticklabels = ['versicolor', 'viginica'],\n",
    "            yticklabels = ['versicolor', 'viginica'],\n",
    "            );\n",
    "\n",
    "plt.ylabel('Actual label', fontsize = 17);\n",
    "plt.xlabel('Predicted label', fontsize = 17);\n",
    "plt.title('Accuracy Score: {:.3f}'.format(score), size = 17);\n",
    "plt.tick_params(labelsize= 15)\n",
    "\n",
    "# You can comment out the next 4 lines if you like\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the score stops improving after a certain number of estimators (decision trees). One way to get a better score would be to include more features in the features matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What would happen if you change the prediction threshold from .5 for picking a positive class</h3>\n",
    "\n",
    "By default, and with respect to the underlying assumptions of logistic regression, we predict a positive class when the probability of the class is greater than .5 and predict a negative class otherwise.\n",
    "\n",
    "If you changed the prediction threshold from .5 to .2, you would predict more true positives but fewer true negatives. You can see this clearly using <a href=\"http://mfviz.com/binary-predictions/\">this visual by Michael Freeman.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What is the effect of changing the hyperparameter C?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the effect of increasing C if you have `l1` regularization. Smaller values specify stronger regularization. The code below shows this for the Wisconsin breast cancer dataset in an effort to mimic Michael Freeman's visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the following file to look at the effect of changing C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video('imagesanimation/effectOfCLogisticRegression.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wisconsinBreastCancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same code was earlier in notebook, but here for clarity\n",
    "# The rest of the lines in this sectionis just code I used to make the animation above\n",
    "\n",
    "col_names = ['worst_concave_points']\n",
    "\n",
    "X = df[col_names].values.reshape(-1,1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "# Standardize Data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,c in enumerate(np.linspace(-3, 3, num = 25)):\n",
    "    \n",
    "    c_value = 10**c\n",
    "    c_value_str = \"{0:0.3f}\".format(c_value)\n",
    "    # Keep in mind that there is l2 penalty by default like we have for ridge regression\n",
    "    logreg = LogisticRegression(C = c_value)\n",
    "    \n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    example_df = pd.DataFrame()\n",
    "    example_df.loc[:, 'worst_concave_points'] = X_train.reshape(-1)\n",
    "    example_df.loc[:, 'diagnosis'] = y_train.values\n",
    "\n",
    "    example_df['logistic_preds'] = pd.DataFrame(logreg.predict_proba(X_train))[1]\n",
    "    example_df = example_df.sort_values(['logistic_preds'])\n",
    "\n",
    "    plt.scatter(example_df['worst_concave_points'], example_df['diagnosis'])\n",
    "    plt.plot(example_df['worst_concave_points'], example_df['logistic_preds'].values, color='red')\n",
    "\n",
    "    plt.ylabel('malignant (1) or benign (0)', fontsize = 13)\n",
    "    plt.xlabel('worst_concave_points', fontsize = 13)\n",
    "    plt.title(\"Logistic Regression (L1) C = \" + str(c_value_str), fontsize = 15)\n",
    "    plt.savefig('imagesanimation/' + 'initial' + str(index).zfill(4) + '.png', dpi = 100)\n",
    "    plt.cla()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What is the effect of regularization on accuracy?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at the video imagesanimation/logisticRegularizationEffectAccuracy.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same code was earlier in notebook, but here for clarity\n",
    "\n",
    "col_names = ['worst_concave_points']\n",
    "\n",
    "X = df[col_names].values.reshape(-1,1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "# Standardize Data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coef_list, c_value_list, accuracy_list, example_df_list = [], [], [], [], []\n",
    "\n",
    "for index,c in enumerate(np.linspace(-3, 3, num = 25)):\n",
    "    \n",
    "    c_value = 10**c\n",
    "    c_value_str = \"{0:0.3f}\".format(c_value)\n",
    "    # Keep in mind that there is l2 penalty by default like we have for ridge regression\n",
    "    logreg = LogisticRegression(C = c_value,\n",
    "                                penalty = 'l1',\n",
    "                                solver = 'saga',\n",
    "                                max_iter = 100000)\n",
    "                                \n",
    "    \n",
    "    logreg.fit(X_train, y_train)\n",
    "    \n",
    "    # Subplot (top)\n",
    "    example_df = pd.DataFrame()\n",
    "    example_df.loc[:, 'worst_concave_points'] = X_train.reshape(-1)\n",
    "    example_df.loc[:, 'diagnosis'] = y_train.values\n",
    "\n",
    "    example_df['logistic_preds'] = pd.DataFrame(logreg.predict_proba(X_train))[1]\n",
    "    example_df = example_df.sort_values(['logistic_preds'])\n",
    "    example_df_list.append(example_df)\n",
    "    model_list.append(logreg)\n",
    "    accuracy_list.append(logreg.score(X_test, y_test))\n",
    "    coef_list.append(logreg.coef_[0])\n",
    "    c_value_list.append(c_value)\n",
    "        \n",
    "temp_df = pd.DataFrame(coef_list, index = c_value_list, columns = col_names)\n",
    "    \n",
    "temp_df.loc[:, 'model'] = model_list\n",
    "\n",
    "# Giving the index a name (it is not a column)\n",
    "temp_df.index.name = 'C (Inverse of Regularization Strength)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (c_value,example_df) in enumerate(zip(c_value_list, example_df_list)):\n",
    "\n",
    "    c_value_str = \"{0:0.3f}\".format(c_value)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows = 2,\n",
    "                             ncols = 1,\n",
    "                             figsize = (12, 7));\n",
    "\n",
    "    # Just formatting, not relevant for this class\n",
    "    fig.subplots_adjust(wspace=0.1, hspace = .55)\n",
    "\n",
    "    \"\"\"\n",
    "    fig.suptitle(\"Logistic Regression (L1) C = \" + str(c_value_str),\n",
    "                 fontsize = 15,\n",
    "                 y=.94)\n",
    "    \"\"\"\n",
    "\n",
    "    # Code is just to make it so you have different colors in the \"title\"\n",
    "    # https://stackoverflow.com/questions/9350171/figure-title-with-several-colors-in-matplotlib\n",
    "    fig.text(0.45,\n",
    "             0.92,\n",
    "             \"Logistic Regression (L1) C = \",\n",
    "             ha=\"center\",\n",
    "             va=\"bottom\",\n",
    "             size=20,\n",
    "             color=\"black\")\n",
    "\n",
    "    fig.text(0.68,\n",
    "             0.92,\n",
    "             str(c_value_str),\n",
    "             ha=\"center\",\n",
    "             va=\"bottom\",\n",
    "             size=20,\n",
    "             color=\"purple\",)\n",
    "\n",
    "    axes[0].scatter(example_df['worst_concave_points'], example_df['diagnosis'])\n",
    "    axes[0].plot(example_df['worst_concave_points'], example_df['logistic_preds'].values, color='red')\n",
    "    axes[0].set_ylabel('malignant (1) or benign (0)', fontsize = 13)\n",
    "    axes[0].set_xlabel('worst_concave_points', fontsize = 11)\n",
    "\n",
    "    axes[1].plot(temp_df.index,\n",
    "                 temp_df.loc[:, 'worst_concave_points'],\n",
    "                 label = 'worst_concave_points',\n",
    "                 color = 'purple');\n",
    "    \n",
    "    axes[1].axvspan(c_value - c_value/10,c_value +  c_value/10, color='orange', alpha=0.3, zorder = 1);\n",
    "\n",
    "    coefLimits = temp_df.min().min(), temp_df.max().max()\n",
    "    accuracyLimits = min(accuracy_list), max(accuracy_list)\n",
    "    \n",
    "    axes[1].tick_params('y', colors='purple');\n",
    "    axes[1].set_ylim(coefLimits)\n",
    "    axes[1].set_yticks(np.linspace(coefLimits[0],coefLimits[1], 11))\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "    axes[1].set_ylabel('weights', color='purple', fontsize = 13)\n",
    "    axes[1].set_xlabel('C', fontsize = 11)\n",
    "\n",
    "    axesTwin=axes[1].twinx()\n",
    "    axesTwin.plot(temp_df.index, accuracy_list, color = 'g')\n",
    "    axesTwin.tick_params('y', colors='g');\n",
    "    axesTwin.set_ylim(accuracyLimits)\n",
    "    axesTwin.set_yticks(np.linspace(accuracyLimits[0],accuracyLimits[1], 11))\n",
    "    axesTwin.set_ylabel('Accuracy', color='g', fontsize = 13); \n",
    "\n",
    "    \n",
    "    axes[1].grid();\n",
    "    ###\n",
    "    \n",
    "    fig.savefig('imagesanimation2/' + 'initial' + str(index).zfill(4) + '.png', dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are really curious, I can share how this works. \n",
    "#!ffmpeg -framerate 1 -i 'initial%04d.png' -c:v libx264 -r 30 -pix_fmt yuv420p initial_002.mp4"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
